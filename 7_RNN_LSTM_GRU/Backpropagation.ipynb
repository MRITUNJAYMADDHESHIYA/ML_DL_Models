{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "665a4268",
   "metadata": {},
   "source": [
    "### BackPropagation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11fadebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import copy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "def6caa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):  #Adds non-linearity and removes negative values, helping neural networks model complex patterns.\n",
    "    return x*(np.sign(x)+1.)/2.\n",
    "def sigmoid(x): #any real number to the range (0, 1)\n",
    "    return 1./(1.+np.exp(-x))\n",
    "def softmax(x): #converts a vector of values into a probability distribution\n",
    "    return np.exp(x)/sum(np.exp(x))\n",
    "def mynorm(z): #Root Mean Square (RMS)\n",
    "    return np.sqrt(np.mean(z**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8729c0",
   "metadata": {},
   "source": [
    "###### A three layer feed-forward network as in the Example given in Chapter 4. On each pass through the training loop, the training input is fed forward through the network to calculate the loss, then the gradient of the loss with respect to each of the weights is calculated and the weights updated for the next pass through the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fb4d67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y->training output\n",
    "#Xtrain->training input\n",
    "#Xpred->input for prediction\n",
    "\n",
    "def myANN(Y, Xtrain, Xpred, w01, w02, w03, b01, b02, b03):\n",
    "    #Initialization\n",
    "    w1 = copy.copy(w01)\n",
    "    w2 = copy.copy(w02)\n",
    "    w3 = copy.copy(w03)\n",
    "    b1 = copy.copy(b01)\n",
    "    b2 = copy.copy(b02)\n",
    "    b3 = copy.copy(b03)\n",
    "\n",
    "    k=1\n",
    "    change = 999\n",
    "\n",
    "    #training loop\n",
    "    while(change>0.001 and k<201):\n",
    "        print('Iteration:', k)\n",
    "\n",
    "        #start feedforward\n",
    "        z1 = sigmoid(w1 @ Xtrain + b1)  #hidden layer 1\n",
    "        z2 = sigmoid(w2 @ z1 + b2)      #hidden layer 2\n",
    "        Yhat = w3 @ z2 + b3             #output layer\n",
    "        loss = -Y @ np.log(Yhat)        #cross-entropy loss\n",
    "        print(\"current loss:\", loss)\n",
    "\n",
    "        ##find gradient of loss w.r.t. each weight\n",
    "        #output layer\n",
    "        dLdb3 = Yhat - Y \n",
    "        dLdW3 = np.outer(dLdb3, z2)\n",
    "        # Hidden Layer 2\n",
    "        dLdb2 = (w3.T @ (dLdb3)) * z2 * (1-z2)\n",
    "        dLdW2 = np.outer(dLdb2,z1)\n",
    "        # Hidden Layer 1\n",
    "        dLdb1 = (w2.T @ (dLdb2)) * z1 * (1-z1)\n",
    "        dLdW1 = np.outer(dLdb1, Xtrain)\n",
    "        \n",
    "        ## Update Weights by Back Propagation\n",
    "        # Output Layer\n",
    "        b3 -= dLdb3 # (learning rate is one)\n",
    "        w3 -= dLdW3\n",
    "        # Hidden Layer 2\n",
    "        b2 -= dLdb2\n",
    "        w2 -= dLdW2\n",
    "        # Hidden Layer 1\n",
    "        b1 -= dLdb1\n",
    "        w1 -= dLdW1\n",
    "\n",
    "        change = norm(dLdb1)+norm(dLdb2)+norm(dLdb3)+norm(dLdW1)+norm(dLdW2)+norm(dLdW3)\n",
    "        k += 1\n",
    "        \n",
    "    Z1pred = w1 @ Xpred + b1\n",
    "    Z2pred = w2 @ sigmoid(Z1pred) + b2\n",
    "    Z3pred = w3 @ sigmoid(Z2pred) + b3\n",
    "    Ypred = softmax(Z3pred)\n",
    "    print(\"\")\n",
    "    print(\"Summary\")\n",
    "    print(\"Target Y \\n\", Y)\n",
    "    print(\"Fitted Ytrain \\n\", Yhat)\n",
    "    print(\"Xpred\\n\", Xpred)\n",
    "    print(\"Fitted Ypred \\n\", Ypred)\n",
    "    print(\"Weight Matrix 1 \\n\", w1)\n",
    "    print(\"Bias Vector 1 \\n\", b1)\n",
    "    print(\"Weight Matrix 2 \\n\", w2)\n",
    "    print(\"Bias Vector 2 \\n\", b2)\n",
    "    print(\"Weight Matrix 3 \\n\", w3)\n",
    "    print(\"Bias Vector 3 \\n\", b3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62dbefe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initial weights and biases\n",
    "W0_1 = np.array([[0.1,0.3,0.7], [0.9,0.4,0.4]])\n",
    "b_1 = np.array([1.,1.])\n",
    "\n",
    "W0_2 = np.array([[0.4,0.3], [0.7,0.2]])\n",
    "b_2 = np.array([1.,1.])\n",
    "\n",
    "W0_3 = np.array([[0.5,0.6], [0.6,0.7], [0.3,0.2]])\n",
    "b_3 = np.array([1.,1.,1.]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8319295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data\n",
    "X_train = np.array([0.1, 0.7, 0.3])\n",
    "YY      = np.array([1., 0., 0.])\n",
    "X_pred  = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9cd0c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "current loss: -0.6539992473098665\n",
      "Iteration: 2\n",
      "current loss: nan\n",
      "Iteration: 3\n",
      "current loss: -0.4805511383308699\n",
      "Iteration: 4\n",
      "current loss: nan\n",
      "Iteration: 5\n",
      "current loss: -0.007541664749705356\n",
      "Iteration: 6\n",
      "current loss: nan\n",
      "\n",
      "Summary\n",
      "Target Y \n",
      " [1. 0. 0.]\n",
      "Fitted Ytrain \n",
      " [ 9.99707392e-01 -2.66237750e-04 -5.08312047e-05]\n",
      "Xpred\n",
      " [0.1 0.7 0.3]\n",
      "Fitted Ypred \n",
      " [0.57611829 0.2119417  0.21194001]\n",
      "Weight Matrix 1 \n",
      " [[0.09153201 0.24072407 0.67459603]\n",
      " [0.90750974 0.45256817 0.42252921]]\n",
      "Bias Vector 1 \n",
      " [0.91532011 1.07509738]\n",
      "Weight Matrix 2 \n",
      " [[-0.92683361 -1.06146774]\n",
      " [-0.65490402 -1.19372961]]\n",
      "Bias Vector 2 \n",
      " [-0.67822254 -0.71821671]\n",
      "Weight Matrix 3 \n",
      " [[ 0.51128657  0.60029999]\n",
      " [ 0.41104025  0.44653699]\n",
      " [ 0.10371677 -0.05263121]]\n",
      "Bias Vector 3 \n",
      " [ 0.89457823 -0.08124117 -0.00440959]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\TEMP.txt\\ipykernel_15092\\1151598572.py:25: RuntimeWarning: invalid value encountered in log\n",
      "  loss = -Y @ np.log(Yhat)        #cross-entropy loss\n"
     ]
    }
   ],
   "source": [
    "myANN(YY, X_train, X_pred, W0_1, W0_2, W0_3, b_1, b_2, b_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90beffff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
